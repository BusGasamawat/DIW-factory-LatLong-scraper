{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c905b8bd",
   "metadata": {},
   "source": [
    "**Factory latlong web scraper (by type of factory)**\n",
    "\n",
    "Data from กรมโรงงานอุตสาหกรรม at https://www.diw.go.th/webdiw/search-factory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important setting\n",
    "fac_type = \"00503\" #รหัสประเภทโรงงาน from https://www.diw.go.th/datahawk/factype.php\n",
    "#optional settings\n",
    "sleep_time1 = 5 #time to wait for page to load after clicking page number, default = 5 (value too low may cause errors)\n",
    "sleep_time2 = 1 #time to wait between extracting each link, default = 1 (value too low may cause server to block IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = []\n",
    "\n",
    "#open main website, input factory type, and click search\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.diw.go.th/webdiw/search-factory/')\n",
    "wait = WebDriverWait(driver, 30)\n",
    "original_window = driver.current_window_handle\n",
    "\n",
    "#accept cookie button\n",
    "element = driver.find_element(By.XPATH, '//*[@id=\"cn-accept-cookie\"]').click()\n",
    "\n",
    "#input factory type and so on\n",
    "element = driver.find_element(By.XPATH, '//*[@id=\"content\"]/article/div/form/table/tbody/tr[5]/td[2]/input')\n",
    "element.send_keys(fac_type)\n",
    "time.sleep(2)\n",
    "element2 = driver.find_element(By.XPATH, '/html/body/div[1]/div/main/div/div/div/article/div/form/table/tbody/tr[16]/td/input[2]').click()\n",
    "\n",
    "wait.until(EC.number_of_windows_to_be(2))\n",
    "\n",
    "for window_handle in driver.window_handles:\n",
    "    if window_handle != original_window:\n",
    "        driver.switch_to.window(window_handle)\n",
    "        break\n",
    "wait.until(EC.title_is(\"Factory Results From Query\"))\n",
    "\n",
    "\n",
    "#start scraping data on new tab\n",
    "\n",
    "#find number of pages\n",
    "page_no = driver.find_elements(By.NAME, \"pageno\")\n",
    "num_pages = 1\n",
    "for i in page_no:\n",
    "    num_pages+=1\n",
    "    text = i.text \n",
    "print(\"Number of pages: \", num_pages, \" Time spent per page: \", sleep_time1, 'seconds')\n",
    "\n",
    "\n",
    "elems = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "for elem in elems:\n",
    "    web_data.append(elem.get_attribute(\"href\"))\n",
    "\n",
    "\n",
    "#scrape all links on all pages using for loop\n",
    "for i in range(2, num_pages + 1):\n",
    "    print(\"Extracting from page: \", i)\n",
    "    xpath_selector = f\"//input[@name='pageno' and @value='{i}']\"\n",
    "    current_page = driver.find_element(By.XPATH, xpath_selector).click()\n",
    "    time.sleep(sleep_time1)\n",
    "\n",
    "    elems = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "    for elem in elems:\n",
    "        web_data.append(elem.get_attribute(\"href\"))\n",
    "driver.quit()\n",
    "print(\"Link extraction complete! Now will proceed to extract factory data.\")\n",
    "\n",
    "#remove unwanted links\n",
    "web_data = [sub for sub in web_data if not any(ele in sub for ele in ['userdb.diw.go.th/query.html'])]\n",
    "print(\"Number of links: \", len(web_data), \" Time spent per link: \", sleep_time2, 'seconds')\n",
    "\n",
    "#beautifulsoup for factory lat long extraction\n",
    "wdata = []\n",
    "for url in web_data:\n",
    "    time.sleep(sleep_time2)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    soup = soup.find(\"body\")\n",
    "    num = 0\n",
    "    for line in soup:\n",
    "        line = str(line)\n",
    "        if num == 0: #fac_id\n",
    "            fac_id = line.split(\" \")[2]\n",
    "        elif num == 1: #lat long\n",
    "            latlong = line.split('N')\n",
    "            latlong = [item.replace(\" \", \"\") for item in latlong]\n",
    "            latlong = latlong[-1].split(\"<\")[0]\n",
    "            latlong = latlong.split('E')\n",
    "        elif num == 4: #fac_name\n",
    "            fac_name = line.split(\":\")[-1].strip()\n",
    "        num += 1\n",
    "    wdata.append([fac_id, fac_name, fac_type, latlong[0], latlong[1], url])\n",
    "\n",
    "#export result to excel\n",
    "print(\"Exporting data to excel...\")\n",
    "fac_data = pd.DataFrame(wdata, columns=['fac_id', 'fac_name', 'fac_type', 'lat', 'long', 'link'])\n",
    "fac_data.to_excel(f'factory_latlong_type{fac_type}.xlsx', index=False)\n",
    "\n",
    "print(\"Factories extraction complete! with a total number of \", len(fac_data), \" factories\")\n",
    "print(fac_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
